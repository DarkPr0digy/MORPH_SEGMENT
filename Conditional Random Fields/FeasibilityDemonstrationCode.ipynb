{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Conditional Random Fields for Surface Segmentation Feasibility Demo 18/08/2020\n",
    "## Tumi Moeng"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Cleaning\n",
    "First, the data we received needs to be cleaned and the surface segmentation\n",
    " forms need to be generated to be used by the models. To this end I created a\n",
    " class called 'DataCleaner' which takes the data we received and filters out\n",
    " that which we need from that which we dont need and generates some more data\n",
    " that we need.<br>\n",
    " This class takes the data in the following form:<br>\n",
    " ngezinkonzo&emsp;khonzo&emsp;P&emsp;[RelConc]-nga[NPre]-i[NPrePre]-zin[BPre]-konzo[NStem]<br>\n",
    " And converts it to the following form:<br>\n",
    " ngezinkonzo | nge-zin-konzo | nga[NPre]i[NPrePre]zin[BPre]konzo[NStem]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from morphology.DataCleaner import DataCleaner\n",
    "languages = [\"zulu\", \"swati\", \"ndebele\", \"xhosa\"]\n",
    "\n",
    "for lang in languages:\n",
    "    print(\"Language: \" + lang)\n",
    "    inputFile = DataCleaner(\"morphology\\\\\"+lang + \".train.conll\")\n",
    "    inputFile.reformat(\"morphology\\\\\"+lang + \".clean.train\")\n",
    "    inputFile = DataCleaner(\"morphology\\\\\"+lang + \".test.conll\")\n",
    "    inputFile.reformat(\"morphology\\\\\"+lang + \".clean.test\")\n",
    "    print(lang + \" cleaning complete.\\n#############################################\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: zulu\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'zulu.train.conll'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-4b313527d7ab>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mmorphology\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataCleaner\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mDataCleaner\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mlanguages\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;34m\"zulu\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"swati\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"ndebele\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"xhosa\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mlang\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mlanguages\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Language: \"\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mlang\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\UniWork\\Fourth Year\\CS Honours\\Honours Project\\CRFModels\\morphology\\DataCleaner.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    423\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mlang\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mlanguages\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    424\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Language: \"\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mlang\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 425\u001B[1;33m     \u001B[0minputFile\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mDataCleaner\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlang\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m\".train.conll\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    426\u001B[0m     \u001B[0minputFile\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlang\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m\".clean.train\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    427\u001B[0m     \u001B[0minputFile\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mDataCleaner\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlang\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m\".test.conll\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\UniWork\\Fourth Year\\CS Honours\\Honours Project\\CRFModels\\morphology\\DataCleaner.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, filename)\u001B[0m\n\u001B[0;32m      9\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfilename\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m         \u001B[1;31m# Open file for reading only\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 11\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfile\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"r\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     12\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlines\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfile\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreadlines\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0ms\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSymbols\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'zulu.train.conll'"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparation for Hyper-parameter Optimisation\n",
    "Secondly, in preparation for the hyper-parameter optimisation that we will need to do\n",
    " to ensure our models are as good as possible we needed to be able to develop a test /\n",
    " validation set to be used in the optimisation process. To this end, I created a class\n",
    " called 'ValidationSetCreation' that extracts about 10% of the data from the training set\n",
    " and puts it into a validation set. This 10% represents roughly the same amount of entries\n",
    " as the test set contains."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: zulu\n",
      "3233\n",
      "zulu validation set complete.\n",
      "#############################################\n",
      "Language: swati\n",
      "5159\n",
      "swati validation set complete.\n",
      "#############################################\n",
      "Language: ndebele\n",
      "2985\n",
      "ndebele validation set complete.\n",
      "#############################################\n",
      "Language: xhosa\n",
      "3262\n",
      "xhosa validation set complete.\n",
      "#############################################\n"
     ]
    }
   ],
   "source": [
    "from morphology.ValidationSetCreation import ValidationSet\n",
    "\n",
    "for lang in languages:\n",
    "    print(\"Language: \" + lang)\n",
    "    file_name =\"morphology\\\\\" + lang + \".clean.train.conll\"\n",
    "    # print(file_name)\n",
    "    inputFile = ValidationSet(file_name)\n",
    "    file_name = \"morphology\\\\\"+lang + \".clean.dev.conll\"\n",
    "    # print(file_name)\n",
    "    inputFile.create_validation_set(file_name)\n",
    "    print(lang + \" validation set complete.\\n#############################################\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conditional Random Field using SKLearn_CRFSuite\n",
    "Finally, we are now able to train the CRF using the data we have developed and\n",
    "train the CRF on this data. We give it input as the string of the language it\n",
    "is to be trained on and it will give 2 outputs. The first is a list of lists of\n",
    "what the CRF predicted the labels will be for the words in the test set. The second\n",
    " is a list of lists of what the actual labels for the word in the test set are. Both\n",
    " inner lists of labels occur in the following form:<br>\n",
    " For the word 'komthombo' which segments to 'ko-m-thombo the inner list would be\n",
    " [B,E,S,B,M,M,M,M,E] where:<br>\n",
    " B = Beginning of Segment<br>\n",
    " M = Middle of Segment<br>\n",
    " E = End of Segment<br>\n",
    " S = Single Length Segment<br>\n",
    " The results method which takes in the list of predicted answers and actual answers\n",
    " can be used to print to console the results of the CRF such as the <b>precision</b>,\n",
    " <b>recall</b> and <b>F1-Score</b>."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Collected in -1.0572801000089385\n",
      "Beginning Feature Computation and Model Optimisation\n",
      "Features Successfully Computed & Model Optimised -0.0006557000451721251\n",
      "X_Training: [{'uni_u': 1, 'bi_uy': 1, 'vowel': 1, 'lower': 1}, {'uni_y': 1, 'bi_uy': 1, 'bi_yi': 1, 'tri_uyi': 1, 'four_uyik': 1, 'const': 1, 'lower': 1}, {'uni_i': 1, 'bi_yi': 1, 'bi_ik': 1, 'tri_yik': 1, 'four_yikh': 1, 'four_uyik': 1, 'five_uyikh': 1, 'six_uyikho': 1, 'vowel': 1, 'lower': 1}, {'uni_k': 1, 'bi_ik': 1, 'bi_kh': 1, 'tri_ikh': 1, 'four_ikho': 1, 'four_yikh': 1, 'five_yikho': 1, 'six_uyikho': 1, 'six_yikhok': 1, 'const': 1, 'lower': 1}, {'uni_h': 1, 'bi_kh': 1, 'bi_ho': 1, 'tri_kho': 1, 'four_khok': 1, 'four_ikho': 1, 'five_ikhok': 1, 'six_yikhok': 1, 'six_ikhokh': 1, 'const': 1, 'lower': 1}, {'uni_o': 1, 'bi_ho': 1, 'bi_ok': 1, 'tri_hok': 1, 'four_hokh': 1, 'four_khok': 1, 'five_khokh': 1, 'six_ikhokh': 1, 'six_khokhe': 1, 'vowel': 1, 'lower': 1}, {'uni_k': 1, 'bi_ok': 1, 'bi_kh': 1, 'tri_okh': 1, 'four_okhe': 1, 'four_hokh': 1, 'five_hokhe': 1, 'six_khokhe': 1, 'six_hokhe': 1, 'const': 1, 'lower': 1}, {'uni_h': 1, 'bi_kh': 1, 'bi_he': 1, 'tri_khe': 1, 'four_khe': 1, 'four_okhe': 1, 'five_okhe': 1, 'six_hokhe': 1, 'six_okhe': 1, 'const': 1, 'lower': 1}, {'uni_e': 1, 'bi_he': 1, 'bi_e': 1, 'tri_he': 1, 'four_he': 1, 'four_khe': 1, 'five_khe': 1, 'six_okhe': 1, 'six_khe': 1, 'vowel': 1, 'lower': 1}]\n",
      "################################\n",
      "Y_training: ['S', 'B', 'E', 'B', 'M', 'M', 'M', 'E', 'B']\n",
      "################################\n",
      "Words Training: ['u', 'y', 'i', 'k', 'h', 'o', 'k', 'h', 'e']\n",
      "############################\n",
      "First Value of Predicted Answer List: ['S', 'B', 'M', 'M', 'M']\n",
      "First Value of Actual Answer List: ['S', 'B', 'M', 'M', 'M']\n",
      "\n",
      "Evaluation on the Test set\n",
      "\n",
      "delta = 8\tepsilon = 1e-07\tmax_iter = 80\tBest Algo = ap\n",
      "Precision = 0.8176916932907349\n",
      "Recall = 0.8304603528696005\n",
      "F1-score = 0.8240265620283731\n",
      "0.818\t0.83\t0.824\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead - the MultiLabelBinarizer transformer can convert to this format.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-5-4be5094a3ef1>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"First Value of Actual Answer List: \"\u001B[0m\u001B[1;33m+\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mactual_ans\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[0mcrf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msurface_segmentation_results\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpredicted_ans\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mactual_ans\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 10\u001B[1;33m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maccuracy_score\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpredicted_ans\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mactual_ans\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mc:\\users\\tumi\\desktop\\applications\\opencv\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py\u001B[0m in \u001B[0;36minner_f\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     70\u001B[0m                           FutureWarning)\n\u001B[0;32m     71\u001B[0m         \u001B[0mkwargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m{\u001B[0m\u001B[0mk\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0marg\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mk\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marg\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 72\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     73\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0minner_f\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     74\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\tumi\\desktop\\applications\\opencv\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001B[0m in \u001B[0;36maccuracy_score\u001B[1;34m(y_true, y_pred, normalize, sample_weight)\u001B[0m\n\u001B[0;32m    185\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    186\u001B[0m     \u001B[1;31m# Compute accuracy for each possible representation\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 187\u001B[1;33m     \u001B[0my_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_true\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_pred\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_check_targets\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_true\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_pred\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    188\u001B[0m     \u001B[0mcheck_consistent_length\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_true\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_pred\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msample_weight\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    189\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0my_type\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'multilabel'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\tumi\\desktop\\applications\\opencv\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001B[0m in \u001B[0;36m_check_targets\u001B[1;34m(y_true, y_pred)\u001B[0m\n\u001B[0;32m     80\u001B[0m     \"\"\"\n\u001B[0;32m     81\u001B[0m     \u001B[0mcheck_consistent_length\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_true\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_pred\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 82\u001B[1;33m     \u001B[0mtype_true\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtype_of_target\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_true\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     83\u001B[0m     \u001B[0mtype_pred\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtype_of_target\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_pred\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     84\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\tumi\\desktop\\applications\\opencv\\venv\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001B[0m in \u001B[0;36mtype_of_target\u001B[1;34m(y)\u001B[0m\n\u001B[0;32m    261\u001B[0m         if (not hasattr(y[0], '__array__') and isinstance(y[0], Sequence)\n\u001B[0;32m    262\u001B[0m                 and not isinstance(y[0], str)):\n\u001B[1;32m--> 263\u001B[1;33m             raise ValueError('You appear to be using a legacy multi-label data'\n\u001B[0m\u001B[0;32m    264\u001B[0m                              \u001B[1;34m' representation. Sequence of sequences are no'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    265\u001B[0m                              \u001B[1;34m' longer supported; use a binary array or sparse'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead - the MultiLabelBinarizer transformer can convert to this format."
     ]
    }
   ],
   "source": [
    "import sklearn_crfsuite\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from BaselineCRF import BaselineCRF\n",
    "crf = BaselineCRF(\"zulu\")\n",
    "predicted_ans, actual_ans = crf.surface_segmentation()\n",
    "print(\"First Value of Predicted Answer List: \"+str(predicted_ans[0]))\n",
    "print(\"First Value of Actual Answer List: \"+str(actual_ans[0]))\n",
    "crf.surface_segmentation_results(predicted_ans, actual_ans)\n",
    "print(accuracy_score(predicted_ans, actual_ans))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__file__\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score([0,1,0,1,1,1,1],[1,1,0,1,0,1,1]))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}