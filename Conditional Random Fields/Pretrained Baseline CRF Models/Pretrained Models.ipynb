{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Baseline CRF Pretrained Models\n",
    "## Tumi Moeng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Representation: ['B', 'M', 'E', 'B', 'M', 'E', 'B', 'M', 'M', 'M', 'E']\n",
      "Segmented Word: nge-zin-konzo\n",
      "Segmentation Representation: ['B', 'M', 'E', 'B', 'E', 'S']\n",
      "Segmented Word: nge-nj-i\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import sklearn_crfsuite\n",
    "\n",
    "# Method to generate features\n",
    "def surface_segment_data_active_preparation(word_list: [str]):\n",
    "    X = []\n",
    "    for word in word_list:\n",
    "        word_list = []\n",
    "        for i in range(len(word)):\n",
    "            gram_dict = {}\n",
    "            gram_arr = []\n",
    "\n",
    "            ### Unigram\n",
    "            # gram_dict[word[i]] = 1\n",
    "            gram_dict[\"uni_\" + word[i]] = 1\n",
    "            gram_arr.append(word[i])\n",
    "\n",
    "            ### BIGRAM\n",
    "            try:\n",
    "                tmp = word[i - 1: i + 1]\n",
    "                if tmp:\n",
    "                    # gram_dict[tmp] = 1\n",
    "                    if len(tmp) == 2:\n",
    "                        gram_dict[\"bi_\" + tmp] = 1\n",
    "                        gram_arr.append(tmp)\n",
    "            except IndexError:\n",
    "                continue\n",
    "            try:\n",
    "                tmp = word[i: i + 2]\n",
    "                if tmp:\n",
    "                    # gram_dict[tmp] = 1\n",
    "                    if len(tmp) == 2:\n",
    "                        gram_dict[\"bi_\" + tmp] = 1\n",
    "                        gram_arr.append(tmp)\n",
    "            except IndexError:\n",
    "                continue\n",
    "\n",
    "            ### TRIGRAM\n",
    "            try:\n",
    "                tmp = word[i - 1: i + 2]\n",
    "                if tmp:\n",
    "                    # gram_dict[tmp] = 1\n",
    "                    if len(tmp) == 3:\n",
    "                        gram_dict[\"tri_\" + tmp] = 1\n",
    "                        gram_arr.append(tmp)\n",
    "            except IndexError:\n",
    "                continue\n",
    "\n",
    "            ##  FourGram\n",
    "            try:\n",
    "                tmp = word[i - 1: i + 3]\n",
    "                if tmp:\n",
    "                    # gram_dict[tmp] = 1\n",
    "                    if len(tmp) == 4:\n",
    "                        gram_dict[\"four_\" + tmp] = 1\n",
    "                        gram_arr.append(tmp)\n",
    "            except IndexError:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                tmp = word[i - 2: i + 2]\n",
    "                if tmp:\n",
    "                    # gram_dict[tmp] = 1\n",
    "                    if len(tmp) == 4:\n",
    "                        gram_dict[\"four_\" + tmp] = 1\n",
    "                        gram_arr.append(tmp)\n",
    "            except IndexError:\n",
    "                continue\n",
    "\n",
    "            ## FiveGram\n",
    "            try:\n",
    "                tmp = word[i - 2: i + 3]\n",
    "                if tmp:\n",
    "                    # gram_dict[tmp] = 1\n",
    "                    if len(tmp) == 5:\n",
    "                        gram_dict[\"five_\" + tmp] = 1\n",
    "                        gram_arr.append(tmp)\n",
    "            except IndexError:\n",
    "                continue\n",
    "\n",
    "            ## SixGram\n",
    "            try:\n",
    "                tmp = word[i - 3: i + 3]\n",
    "                if tmp:\n",
    "                    if len(tmp) == 6:\n",
    "                        # gram_dict[tmp] = 1\n",
    "                        gram_dict[\"six_\" + tmp] = 1\n",
    "                        gram_arr.append(tmp)\n",
    "            except IndexError:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                tmp = word[i - 2: i + 4]\n",
    "                if tmp:\n",
    "                    if len(tmp) == 6:\n",
    "                        # gram_dict[tmp] = 1\n",
    "                        gram_dict[\"six_\" + tmp] = 1\n",
    "                        gram_arr.append(tmp)\n",
    "            except IndexError:\n",
    "                continue\n",
    "\n",
    "            if word[i] in 'aeiou':\n",
    "                gram_dict[\"vowel\"] = 1\n",
    "            else:\n",
    "                gram_dict[\"const\"] = 1\n",
    "\n",
    "            if word[i].isupper():\n",
    "                gram_dict[\"upper\"] = 1\n",
    "            else:\n",
    "                gram_dict[\"lower\"] = 1\n",
    "\n",
    "            word_list.append(gram_dict)\n",
    "\n",
    "        X.append(word_list)\n",
    "    return X\n",
    "\n",
    "# Load The Models\n",
    "isiNdebeleModel = pickle.load(open('isiNdebeleSurfaceModel.sav', 'rb'))\n",
    "isiXhosaModel = pickle.load(open('isiXhosaSurfaceModel.sav', 'rb'))\n",
    "isiZuluModel = pickle.load(open('isiZuluSurfaceModel.sav', 'rb'))\n",
    "siSwatiModel = pickle.load(open('siSwatiSurfaceModel.sav', 'rb'))\n",
    "\n",
    "langModels = {\"isiNdebele\": isiNdebeleModel, \"isiXhosa\": isiXhosaModel, \"isiZulu\": isiZuluModel, \"siSwati\": siSwatiModel}\n",
    "\n",
    "language = input(\"Select a language or quit(q): \")\n",
    "\n",
    "while language != \"q\":\n",
    "    if language in langModels.keys():\n",
    "        word = input(\"Enter a word: \")\n",
    "        features = surface_segment_data_active_preparation([word])\n",
    "\n",
    "        prediction = langModels[language].predict(features)\n",
    "\n",
    "        labels = prediction[0]\n",
    "        word_list = list(word)\n",
    "\n",
    "        tmp = []\n",
    "        for word, label in zip(word_list, labels):\n",
    "            for i in range(len(label)):\n",
    "                if label[i] == \"S\" or label[i] == \"E\":\n",
    "                    tmp.append(word[i])\n",
    "                    tmp.append(\"-\")\n",
    "                else:\n",
    "                    tmp.append(word[i])\n",
    "        tmp = \"\".join(tmp).rstrip(\"-\")\n",
    "\n",
    "        print(\"Segmentation Representation: \"+str(labels))\n",
    "        print(\"Segmented Word: \"+tmp)\n",
    "\n",
    "    language = input(\"Select a valid language or quit(q): \")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}