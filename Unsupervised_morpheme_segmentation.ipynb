{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unsupervised_morpheme_segmentation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "blY0z3zkq72C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fff4b13-3c93-4233-b616-ed3d436b0aeb"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/') \n",
        " \n",
        "!pip install morfessor\n",
        "import morfessor\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import statistics\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from io import open\n",
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n",
            "Requirement already satisfied: morfessor in /usr/local/lib/python3.7/dist-packages (2.0.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8PAlZ_fyO8f"
      },
      "source": [
        "## Model 1: Morfessor-Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAGnO7Lrti-d"
      },
      "source": [
        "#Model 1: Morfessor-Baseline\n",
        "class baseline:\n",
        "\n",
        "\n",
        "    def __init__(self):\n",
        "        self.info = ''\n",
        "        self.scores = None\n",
        "\n",
        "#Trainer: trains from data in inFile and outputs binary trained model to modelFile\n",
        "    def trainModel(self, inFile, modelFile):\n",
        "        io = morfessor.MorfessorIO()\n",
        "        train_data = list(io.read_corpus_file(inFile))\n",
        "\n",
        "        print('Training now...')\n",
        "        model = morfessor.BaselineModel()\n",
        "        model.load_data(train_data)\n",
        "        model.train_batch()\n",
        "\n",
        "        io.write_binary_model_file(modelFile, model)\n",
        "        \n",
        "#Evaluater: test trained model from modelFile on data at inFile and outputs results to outFile on a given language\n",
        "    def evaluateModel(self, inFile, outFile, modelFile, language, evaluate = True):\n",
        "        io = morfessor.MorfessorIO()\n",
        "        model = io.read_binary_model_file(modelFile)\n",
        "\n",
        "        segmentations = []\n",
        "        f = open(inFile, 'r')\n",
        "        for lines in f:\n",
        "            segmentations.append(model.viterbi_segment(lines.split()[0])[0])\n",
        "\n",
        "        if evaluate == True:\n",
        "            util = utilities()\n",
        "            self.scores = util.evalMorphSegments(segmentations, util.loadEvaluationData(inFile)[1])\n",
        "            f = open(outFile, 'a')\n",
        "            self.info = ('\\n' + language + ':\\nTokens: ' + str(util.loadEvaluationData(inFile)[0]) + '\\nActual segmentations: ' + \n",
        "                        str(util.loadEvaluationData(inFile)[1]) + '\\nPredicted segmentations: ' + \n",
        "                        str(segmentations) + '\\nScores: Precsion: ' + str(self.scores[0]) + \n",
        "                        ' Recall: ' + str(self.scores[1]) + \n",
        "                        ' F-score: ' + str(self.scores[2]) + '\\n')\n",
        "            f.write(self.info)\n",
        "            print(self.info)\n",
        "\n",
        "        return segmentations\n",
        "       \n",
        "\n",
        "    def getScores(self):\n",
        "        return self.scores\n",
        "\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj1EU2H5yXfN"
      },
      "source": [
        "## Useful methods for LM models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E8T-2HoUJfw"
      },
      "source": [
        "#Useful methods for LM1 and LM2\n",
        "class utilities:\n",
        "\n",
        "    #Loads data from corpus\n",
        "    def loadEvaluationData(self, file):\n",
        "        g = []\n",
        "        h = []\n",
        "        f = open(file,'r')\n",
        "        for line in f:\n",
        "            tokens = line.split()\n",
        "            g.append(tokens[0])\n",
        "            temp = []\n",
        "            for j in range(1, len(tokens)):\n",
        "                temp.append(tokens[j])\n",
        "            h.append(temp)\n",
        "        return [g,h]    \n",
        "\n",
        "\n",
        "    #reverses elements in a 2D array with the option to exculde last element per sub array from this reversal\n",
        "    def reverseListElem(self, list, exceptLastElem = False):\n",
        "        revList = []\n",
        "        if exceptLastElem:  \n",
        "            temp = len(list) \n",
        "            for i in range(temp):\n",
        "                revList.append(list[i][::-1][1:])\n",
        "                revList[i].append(list[i][-1]) \n",
        "        else:\n",
        "            temp = len(list) \n",
        "            for i in range(temp):\n",
        "                revList.append(list[i][::-1]) \n",
        "                    \n",
        "        return revList\n",
        "            \n",
        "\n",
        "    #Determines score for a list of segmentations\n",
        "    def evalMorphSegments(self, predicted, target):\n",
        "        correct = 0.0\n",
        "        for pred, targ in zip(predicted, target):\n",
        "            for p in pred:\n",
        "                if p in targ:\n",
        "                    correct += 1\n",
        "\n",
        "        predicted_length = sum([len(pred) for pred in predicted])\n",
        "        target_length = sum([len(targ) for targ in target])\n",
        "        precision, recall = correct/predicted_length, correct/target_length\n",
        "        f_score = 2/(1/precision + 1/recall)\n",
        "        return (precision, recall, f_score)\n",
        "\n",
        "\n",
        "    #computes entropy for a probabilty distribution\n",
        "    def entropy(self, distribution):\n",
        "        if len(distribution) < 1:\n",
        "            return 0\n",
        "\n",
        "        entropy_acc = 0\n",
        "        for probability in distribution:\n",
        "            entropy_acc += probability * math.log(probability,2)\n",
        "        return -entropy_acc \n",
        "\n",
        "\n",
        "    #Segments tokens according to specified objective function\n",
        "    def segmenter(self, tokens, lentropies, rentropies, language, objectiveFunction):\n",
        "        count = 0\n",
        "        morphList = []\n",
        "        for i in range(len(lentropies)):\n",
        "            if objectiveFunction == 1:\n",
        "                segmentList = self.objectiveFunction3(lentropies[i], rentropies[i], language)\n",
        "            elif objectiveFunction == 2:\n",
        "                segmentList = self.objectiveFunction2(lentropies[i], rentropies[i], language)\n",
        "            elif objectiveFunction == 3:\n",
        "                segmentList = self.objectiveFunction3(lentropies[i], rentropies[i], language)\n",
        "            elif objectiveFunction == 4:\n",
        "                segmentList = self.objectiveFunction4(lentropies[i], rentropies[i], language)\n",
        "            elif objectiveFunction == 5:\n",
        "                segmentList = self.objectiveFunction5(lentropies[i], rentropies[i], language)\n",
        "            else: \n",
        "                segmentList = self.objectiveFunction6(lentropies[i], rentropies[i], language)\n",
        "            \n",
        "            s = list(zip(segmentList, tokens[i]))\n",
        "            temp = []\n",
        "            tempString = ''\n",
        "            for j in range(len(s)):\n",
        "                if s[j][0] == 0:\n",
        "                    tempString+=s[j][1]\n",
        "                else:\n",
        "                    tempString+=s[j][1]\n",
        "                    temp.append(tempString)\n",
        "                    tempString = ''\n",
        "\n",
        "            morphList.append(temp)\n",
        "        return morphList\n",
        "\n",
        "\n",
        "    #Segment words at random locations\n",
        "    def objectiveFunction1(self, lwordentropies, rwordentropies, language):\n",
        "        segmentArray = []\n",
        "        if len(lwordentropies) > 1:\n",
        "            for i in range(len(lwordentropies)-1):\n",
        "                segmentArray.append(random.randint(0, 1))\n",
        "        segmentArray.append(1)\n",
        "        return segmentArray\n",
        "\n",
        "\n",
        "    #Segments based on experimentally determined constants based on language\n",
        "    def objectiveFunction2(self, lwordentropies, rwordentropies, language):\n",
        "        segmentArray = []\n",
        "        if language == 'xhosa':\n",
        "            val = 4\n",
        "        elif language == 'zulu':\n",
        "            val = 3\n",
        "        elif language == 'swati':\n",
        "            val = 12\n",
        "        else:\n",
        "            val = 2.5\n",
        "        for i in range(0, len(lwordentropies)-1):\n",
        "            if lwordentropies[i] + rwordentropies[i] > val:\n",
        "                segmentArray.append(1)\n",
        "            else:\n",
        "                segmentArray.append(0)\n",
        "        segmentArray.append(1)\n",
        "        return segmentArray\n",
        "\n",
        "\n",
        "    #Segments using right and left entropy and mean with standard deviation \n",
        "    def objectiveFunction3(self, lwordentropies, rwordentropies, language):\n",
        "        if len(lwordentropies) == 1:\n",
        "            return [1]\n",
        "        elif len(lwordentropies) == 2:\n",
        "            return [0, 1]\n",
        "        elif len(lwordentropies) > 2:\n",
        "            segmentArray = []\n",
        "            numListl = []\n",
        "            numListr = []\n",
        "            for j in range(len(lwordentropies)-1):\n",
        "                numListl.append(lwordentropies[j].item())\n",
        "                numListr.append(rwordentropies[j].item())\n",
        "            meanl = statistics.mean(numListl)\n",
        "            meanr = statistics.mean(numListr)\n",
        "            stdevl = statistics.stdev(numListl)\n",
        "            stdevr = statistics.stdev(numListr)\n",
        "            mean, stdev = (meanl + meanr)/2, (stdevl + stdevr)/2\n",
        "            for i in range(len(numListr)):\n",
        "                if (numListl[i] + numListr[i])/2 <= (mean + 1*stdev):\n",
        "                    segmentArray.append(0)\n",
        "                else:\n",
        "                    segmentArray.append(1)\n",
        "            segmentArray.append(1)\n",
        "            return segmentArray\n",
        "\n",
        "\n",
        "    #Uses entropy at neighbouring positions at a each index to determine a segmentation\n",
        "    def objectiveFunction4(self, lwordentropies, rwordentropies, language):\n",
        "        if len(lwordentropies) == 1:\n",
        "            return [1]\n",
        "        elif len(lwordentropies) == 2:\n",
        "            return [0, 1]\n",
        "        elif len(lwordentropies) > 2:\n",
        "            segmentArray = []\n",
        "            x2 = lwordentropies[1]\n",
        "            x1 = lwordentropies[0]\n",
        "            y2 = rwordentropies[1]\n",
        "            y1 = rwordentropies[0]\n",
        "            if x1 > x2 and y1 > y2:\n",
        "                segmentArray.append(1)\n",
        "            else:\n",
        "                segmentArray.append(0)\n",
        "            \n",
        "            if len(lwordentropies) > 3:\n",
        "                for i in range(1, len(lwordentropies)-2):\n",
        "                    if lwordentropies[i] > lwordentropies[i-1] and lwordentropies[i] > lwordentropies[i+1] or rwordentropies[i] > rwordentropies[i-1] and rwordentropies[i] > rwordentropies[i+1]:\n",
        "                        segmentArray.append(1)\n",
        "                    else:\n",
        "                        segmentArray.append(0)\n",
        "\n",
        "            x2 = lwordentropies[len(lwordentropies)-2]\n",
        "            x1 = lwordentropies[len(lwordentropies)-3]\n",
        "            y2 = rwordentropies[len(lwordentropies)-2]\n",
        "            y1 = rwordentropies[len(lwordentropies)-3]\n",
        "            if x1 > x2 and y1 > y2:\n",
        "                segmentArray.append(1)\n",
        "            else:\n",
        "                segmentArray.append(0)\n",
        "            \n",
        "            segmentArray.append(1)\n",
        "            return segmentArray\n",
        "\n",
        "\n",
        "    #Segmentation is based on the mean and standard deviation of left entropy \n",
        "    def objectiveFunction5(self, lwordentropies, rwordentropies, language):\n",
        "        if len(lwordentropies) == 1:\n",
        "            return [1]\n",
        "        elif len(lwordentropies) == 2:\n",
        "            return [0, 1]\n",
        "        elif len(lwordentropies) > 2:\n",
        "            segmentArray = []\n",
        "            numListl = []\n",
        "            numListr = []\n",
        "            for j in range(len(lwordentropies)-1):\n",
        "                numListl.append(lwordentropies[j].item())\n",
        "                numListr.append(rwordentropies[j].item())\n",
        "            meanl = statistics.mean(numListl)\n",
        "            meanr = statistics.mean(numListr)\n",
        "            stdevl = statistics.stdev(numListl)\n",
        "            stdevr = statistics.stdev(numListr)\n",
        "            mean, stdev = (meanl + meanr)/2, (stdevl + stdevr)/2\n",
        "            for i in range(len(numListr)):\n",
        "                if numListl[i] <= (mean + 1*stdev):\n",
        "                    segmentArray.append(0)\n",
        "                else:\n",
        "                    segmentArray.append(1)\n",
        "            segmentArray.append(1)\n",
        "            return segmentArray\n",
        "    \n",
        "\n",
        "    #Segmentation based on neigbouring positions in a taoken using left entropy only \n",
        "    def objectiveFunction6(self, lwordentropies, rwordentropies, language):\n",
        "        if len(lwordentropies) == 1:\n",
        "            return [1]\n",
        "        elif len(lwordentropies) == 2:\n",
        "            return [0, 1]\n",
        "        elif len(lwordentropies) > 2:\n",
        "            segmentArray = []\n",
        "            x2 = lwordentropies[1]\n",
        "            x1 = lwordentropies[0]\n",
        "            y2 = rwordentropies[1]\n",
        "            y1 = rwordentropies[0]\n",
        "            if x1 > x2 and y1 > y2:\n",
        "                segmentArray.append(1)\n",
        "            else:\n",
        "                segmentArray.append(0)\n",
        "            \n",
        "            if len(lwordentropies) > 3:\n",
        "                for i in range(1, len(lwordentropies)-2):\n",
        "                    if lwordentropies[i] > lwordentropies[i-1] and lwordentropies[i] > lwordentropies[i+1]:\n",
        "                        segmentArray.append(1)\n",
        "                    else:\n",
        "                        segmentArray.append(0)\n",
        "\n",
        "            x2 = lwordentropies[len(lwordentropies)-2]\n",
        "            x1 = lwordentropies[len(lwordentropies)-3]\n",
        "            y2 = rwordentropies[len(lwordentropies)-2]\n",
        "            y1 = rwordentropies[len(lwordentropies)-3]\n",
        "            if x1 > x2 and y1 > y2:\n",
        "                segmentArray.append(1)\n",
        "            else:\n",
        "                segmentArray.append(0)\n",
        "            \n",
        "            segmentArray.append(1)\n",
        "            return segmentArray\n",
        "    \n",
        "\n",
        "    #word segmentation\n",
        "    def objectiveFunction7(self, lwordentropies, rwordentropies, language):\n",
        "        segmentArray = []\n",
        "        for i in range(0, len(lwordentropies)-1):\n",
        "                segmentArray.append(0)\n",
        "        segmentArray.append(1)\n",
        "        return segmentArray"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4FFcgmhL43k"
      },
      "source": [
        "## Model 2: LM1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYC3UyRj7jtf"
      },
      "source": [
        "#Model 2: LM1\n",
        "# Dictionary and Corpus does character indexing for LM1\n",
        "class Dictionary(object):\n",
        "\n",
        "\n",
        "    def __init__(self):\n",
        "        self.char2idx = {}\n",
        "        self.idx2char = []\n",
        " \n",
        "\n",
        "    def add_char(self, char):\n",
        "        if char not in self.char2idx:\n",
        "            self.idx2char.append(char)\n",
        "            self.char2idx[char] = len(self.idx2char) - 1\n",
        "        return self.char2idx[char]\n",
        " \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2char)\n",
        " \n",
        " \n",
        "class Corpus(object):\n",
        "\n",
        "\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(path + 'train.txt')\n",
        "        self.valid = self.tokenize(path + 'valid.txt')\n",
        "        self.test = self.tokenize(path + 'test.txt')\n",
        " \n",
        "\n",
        "    def tokenize(self, path):\n",
        "        '''Tokenizes a text file.'''\n",
        "        assert os.path.exists(path)\n",
        "        # Add chars to the dictionary\n",
        "        with open(path, 'r', encoding='utf8') as f:\n",
        "            for line in f:\n",
        "                chars = line.split() \n",
        "                for char in chars:\n",
        "                    self.dictionary.add_char(char)\n",
        "                    self.dictionary.add_char('%') \n",
        " \n",
        " \n",
        "        # Tokenize file content\n",
        "        with open(path, 'r', encoding='utf8') as f:\n",
        "            idss = []\n",
        "            for line in f:\n",
        "                chars = line.split()\n",
        "                ids = []\n",
        "                for char in chars:\n",
        "                    ids.append(self.dictionary.char2idx[char])\n",
        "                ids.append(self.dictionary.char2idx['%'])\n",
        "                idss.append(torch.tensor(ids).type(torch.int64))\n",
        "            ids = torch.cat(idss)\n",
        " \n",
        "        return ids\n",
        "\n",
        "\n",
        "\n",
        "#LM1 LSTM \n",
        "class RNNModel(nn.Module):\n",
        "    '''Container module with an encoder, a recurrent module, and a decoder.'''\n",
        "\n",
        "\n",
        "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.ntoken = ntoken\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        if rnn_type in ['LSTM', 'GRU']:\n",
        "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
        "        else:\n",
        "            try:\n",
        "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
        "            except KeyError:\n",
        "                raise ValueError( '''An invalid option for `--model` was supplied,\n",
        "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']''')\n",
        "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        if tie_weights:\n",
        "            if nhid != ninp:\n",
        "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        self.init_weights()\n",
        "        self.rnn_type = rnn_type\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.weight)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output)\n",
        "        decoded = decoded.view(-1, self.ntoken)\n",
        "        return F.log_softmax(decoded, dim=1), hidden\n",
        "\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
        "        else:\n",
        "            return weight.new_zeros(self.nlayers, bsz, self.nhid)\n",
        "\n",
        "\n",
        "\n",
        "#LM1 trainer: contains methods necessary for LM1 training \n",
        "class trainModel:\n",
        "\n",
        "\n",
        "    def __init__(self, datasets, save, model ='LSTM', emsize =200, nhid =200, nlayers =2,\n",
        "                lr =20.0, clip =0.25, epochs =1, batch_size =20, bptt =35, dropout =0.2, tied =False, \n",
        "                seed =1111, cuda =False, log_interval =200,  nhead =2,\n",
        "               dry_run =False):\n",
        "\n",
        "        torch.manual_seed(seed)\n",
        "        if torch.cuda.is_available():\n",
        "            if not cuda:\n",
        "                print('WARNING: You have a CUDA device, so you should probably run with --cuda')\n",
        "        device = torch.device('cuda' if cuda else 'cpu')\n",
        "\n",
        "        #Load data\n",
        "        corpus = Corpus(datasets)\n",
        "        eval_batch_size = 10\n",
        "        train_data = self.batchify(corpus.train, batch_size, device)\n",
        "        val_data = self.batchify(corpus.valid, eval_batch_size, device)\n",
        "        test_data = self.batchify(corpus.test, eval_batch_size, device)\n",
        "\n",
        "        # Build the model\n",
        "        ntokens = len(corpus.dictionary)\n",
        "        model = RNNModel(model, ntokens, emsize, nhid, nlayers, dropout, tied).to(device)\n",
        "        criterion = nn.NLLLoss()\n",
        "\n",
        "        # Training code\n",
        "        best_val_loss = None\n",
        "        try:\n",
        "            for epoch in range(1, epochs+1):\n",
        "                epoch_start_time = time.time()\n",
        "                self.train(model, corpus, batch_size, train_data, bptt, criterion, clip, lr, log_interval, dry_run, epoch)\n",
        "                val_loss = self.evaluate(val_data, model, corpus, eval_batch_size, criterion, bptt)\n",
        "                print('-' * 89)\n",
        "                print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                        'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                                   val_loss, math.exp(val_loss)))\n",
        "                print('-' * 89)\n",
        "                if not best_val_loss or val_loss < best_val_loss:\n",
        "                    with open(save, 'wb') as f:\n",
        "                        torch.save(model, f)\n",
        "                    best_val_loss = val_loss\n",
        "                else:\n",
        "                    lr /= 4.0\n",
        "        except KeyboardInterrupt:\n",
        "            print('-' * 89)\n",
        "            print('Exiting from training early')\n",
        "\n",
        "        # Load the best saved model.\n",
        "        with open(save, 'rb') as f:\n",
        "            model = torch.load(f)\n",
        "            if model in ['RNN_TANH', 'RNN_RELU', 'LSTM', 'GRU']:\n",
        "                model.rnn.flatten_parameters()\n",
        "\n",
        "        # Run on test data.\n",
        "        test_loss = self.evaluate(test_data, model, corpus, eval_batch_size, criterion, bptt)\n",
        "        print('=' * 89)\n",
        "        print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "            test_loss, math.exp(test_loss)))\n",
        "        print('=' * 89)\n",
        "\n",
        "\n",
        "    def batchify(self, data, bsz, device):\n",
        "        nbatch = data.size(0) // bsz\n",
        "        data = data.narrow(0, 0, nbatch * bsz)\n",
        "        data = data.view(bsz, -1).t().contiguous()\n",
        "        return data.to(device)\n",
        "\n",
        "\n",
        "    def repackage_hidden(self, h):\n",
        "        if isinstance(h, torch.Tensor):\n",
        "            return h.detach()\n",
        "        else:\n",
        "            return tuple(self.repackage_hidden(v) for v in h)\n",
        "\n",
        "\n",
        "    def get_batch(self, source, i, bptt):\n",
        "        seq_len = min(bptt, len(source) - 1 - i)\n",
        "        data = source[i:i+seq_len]\n",
        "        target = source[i+1:i+1+seq_len].view(-1)\n",
        "        return data, target\n",
        "\n",
        "\n",
        "    def evaluate(self, data_source, model, corpus, eval_batch_size, criterion, bptt):\n",
        "        model.eval()\n",
        "        total_loss = 0.\n",
        "        ntokens = len(corpus.dictionary)\n",
        "        hidden = model.init_hidden(eval_batch_size)\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, data_source.size(0) - 1, bptt):\n",
        "                data, targets = self.get_batch(data_source, i, bptt)\n",
        "                output, hidden = model(data, hidden)\n",
        "                hidden = self.repackage_hidden(hidden)\n",
        "                total_loss += len(data) * criterion(output, targets).item()\n",
        "        return total_loss / (len(data_source) - 1)\n",
        "\n",
        "\n",
        "    def train(self, model, corpus, batch_size, train_data, bptt, criterion, clip, lr, log_interval, dry_run, epoch):\n",
        "        model.train()\n",
        "        total_loss = 0.\n",
        "        start_time = time.time()\n",
        "        ntokens = len(corpus.dictionary)\n",
        "        hidden = model.init_hidden(batch_size)\n",
        "        for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "            data, targets = self.get_batch(train_data, i, bptt)\n",
        "            model.zero_grad()\n",
        "            hidden = self.repackage_hidden(hidden)\n",
        "            output, hidden = model(data, hidden)\n",
        "            loss = criterion(output, targets)\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            for p in model.parameters():\n",
        "                p.data.add_(p.grad, alpha=-lr)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if batch % log_interval == 0 and batch > 0:\n",
        "                cur_loss = total_loss / log_interval\n",
        "                elapsed = time.time() - start_time\n",
        "                print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                        'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                    epoch, batch, len(train_data) // bptt, lr,\n",
        "                    elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
        "                total_loss = 0\n",
        "                start_time = time.time()\n",
        "            if dry_run:\n",
        "                break\n",
        "\n",
        "\n",
        "#LM1 evaluater: uses a given trained and test data to produce and evaluate segmentations\n",
        "class evaluateModel:\t\n",
        "\n",
        "\n",
        "    def __init__(self, datasets, testdata, output, lmodel, rmodel, language, objectiveFunction, evaluate = True):\n",
        "        self.datasets = datasets\n",
        "        self.testdata = testdata\n",
        "        self.lmodel = lmodel\n",
        "        self.rmodel = rmodel\n",
        "        self.util = utilities()\n",
        "        self.tokens = self.util.loadEvaluationData(testdata)[0]\n",
        "        self.actual = self.util.loadEvaluationData(testdata)[1]\n",
        "        self.entropy = self.getEntropy(self.tokens, datasets, lmodel, rmodel)\n",
        "        self.prediction = self.util.segmenter(self.tokens, self.entropy[0], self.entropy[1], language, objectiveFunction) \n",
        "        if evaluate == True:\n",
        "            self.evaluation = self.util.evalMorphSegments(self.prediction, self.actual)\n",
        "            self.info = (language + ':\\nTestdata: ' + str(self.testdata) + '\\nTokens: ' + str(self.tokens) + \n",
        "                '\\nReversed tokens: ' + str(self.util.reverseListElem(self.tokens)) + '\\nActual segmentation: ' + \n",
        "                str(self.actual) + '\\nPrediction: ' + str(self.prediction) + '\\nLeft entropy: ' + \n",
        "                str(self.entropy[0]) + '\\nRight entropy: ' + str(self.entropy[1]) + '\\nScores: '+ \n",
        "                'Precision: ' + str(self.evaluation[0]) + ' Recall: ' + str(self.evaluation[1]) + \n",
        "                ' F-Score: ' + str(self.evaluation[2]) + '\\n' + '-'*10000 + '\\n')\n",
        "            f = open(output, 'a')\n",
        "            f.write(self.info)\n",
        "\n",
        "\n",
        "    def getEntropy(self, inf, datasets, lcheckpoint, rcheckpoint, cuda = False, seed = 1111, temperature = 1.0):\n",
        "        # Set the random seed manually for reproducibility.\n",
        "        torch.manual_seed(seed)\n",
        "        if torch.cuda.is_available():\n",
        "            if not cuda:\n",
        "                print('WARNING: You have a CUDA device, so you should probably run with --cuda')\n",
        "        device = torch.device('cuda' if cuda else 'cpu')\n",
        "        if temperature < 1e-3:\n",
        "            print('--temperature has to be greater or equal 1e-3')\n",
        "            sys.exit()\n",
        "        \n",
        "        f = open(lcheckpoint, 'rb')\n",
        "        leftModel = torch.load(f).to(device)\n",
        "        leftModel.eval()\n",
        "\n",
        "        f = open(rcheckpoint, 'rb')\n",
        "        rightModel = torch.load(f).to(device)\n",
        "        rightModel.eval()\n",
        "    \n",
        "        corpus = Corpus(datasets)\n",
        "        ntokens = len(corpus.dictionary)\n",
        "\n",
        "        leftEntropy = []\n",
        "        rightEntropy = []\n",
        "        i = 0\n",
        "\n",
        "        with torch.no_grad():  # no tracking history\n",
        "            for words in inf:\n",
        "                words = words.strip()           \n",
        "                leftEntropy.append([])\n",
        "                rightEntropy.append([])\n",
        "                lefthidden = leftModel.init_hidden(1)\n",
        "                righthidden = rightModel.init_hidden(1)\n",
        "                leftinput = torch.tensor([[corpus.dictionary.char2idx[words[0]]]], dtype=torch.long).to(device) \n",
        "                rightinput = torch.tensor([[corpus.dictionary.char2idx[words[-1]]]], dtype=torch.long).to(device) \n",
        "\n",
        "                for j in range(len(words)-1):\n",
        "                    #left model\n",
        "                    output, lefthidden = leftModel(leftinput, lefthidden)\n",
        "                    char_weights = output.squeeze().div(temperature).exp().cpu()\n",
        "                    leftEntropy[i].append(self.util.entropy(char_weights[:]))\n",
        "                    leftinput.fill_(corpus.dictionary.char2idx[words[j]])\n",
        "\n",
        "                    #right model\n",
        "                    output, righthidden = rightModel(rightinput, righthidden)\n",
        "                    char_weights = output.squeeze().div(temperature).exp().cpu()\n",
        "                    rightEntropy[i].append(self.util.entropy(char_weights[:]))\n",
        "                    rightinput.fill_(corpus.dictionary.char2idx[words[len(words)-j-1]])\n",
        "\n",
        "                leftEntropy[i].append('$')\n",
        "                rightEntropy[i].append('$')\n",
        "                i += 1\n",
        "\n",
        "        return (leftEntropy, self.util.reverseListElem(rightEntropy, True))\n",
        "\n",
        "    \n",
        "    def getScores(self):\n",
        "        return self.evaluation\n",
        "\n",
        "\n",
        "    def getInfo(self):\n",
        "        return self.info\n",
        "\n",
        "    def getPredictedSegmentations(self):\n",
        "        return self.prediction\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0X1emz8zczi"
      },
      "source": [
        "## Model 3: LM2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbHYqWKszPrP"
      },
      "source": [
        "#Model 3: LM2 \n",
        "#Converts sequences to vectors that the LSTM can intepret \n",
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot\n",
        "\n",
        "\n",
        "#LM2 LSTM\n",
        "class CharRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, tokens, n_steps =100, n_hidden =512, n_layers =2, drop_prob =0.5, lr =0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        # Creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        ## Define the LSTM\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        ## Define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        ## Define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "        \n",
        "        # Initialize the weights\n",
        "        self.init_weights()\n",
        "      \n",
        "    \n",
        "    def forward(self, x, hc):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the hidden/cell state `hc`. '''\n",
        "        \n",
        "        ## Get x, and the new hidden state (h, c) from the lstm\n",
        "        x, (h, c) = self.lstm(x, hc)\n",
        "        \n",
        "        ## Ppass x through the dropout layer\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # Stack up LSTM outputs using view\n",
        "        x = x.contiguous().view(-1, self.n_hidden)\n",
        "        ## Put x through the fully-connected layer\n",
        "        x = self.fc(x)\n",
        "        \n",
        "        # Return x and the hidden state (h, c)\n",
        "        return x, (h, c)\n",
        "    \n",
        "    \n",
        "    def predict(self, char, h=None, cuda=False, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "        \n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        if cuda:\n",
        "            self.cuda()\n",
        "        else:\n",
        "            self.cpu()\n",
        "        \n",
        "        if h is None:\n",
        "            h = self.init_hidden(1)\n",
        "        \n",
        "        x = np.array([[self.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(self.chars))\n",
        "        \n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if cuda:\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        h = tuple([each.data for each in h])\n",
        "        out, h = self.forward(inputs, h)\n",
        " \n",
        "        p = F.softmax(out, dim=1).data\n",
        "        \n",
        "        if cuda:\n",
        "            p = p.cpu()\n",
        "        \n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(self.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        p = p.numpy().squeeze()\n",
        "        \n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "            \n",
        "        return p, h\n",
        "    \n",
        " \n",
        "    def init_weights(self):\n",
        "        ''' Initialize weights for fully connected layer '''\n",
        "        initrange = 0.1\n",
        "        \n",
        "        # Set bias tensor to all zeros\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        # FC weights as random uniform\n",
        "        self.fc.weight.data.uniform_(-1, 1)\n",
        "        \n",
        " \n",
        "    def init_hidden(self, n_seqs):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        return (weight.new(self.n_layers, n_seqs, self.n_hidden).zero_(),\n",
        "                weight.new(self.n_layers, n_seqs, self.n_hidden).zero_())\n",
        "        \n",
        "\n",
        "\n",
        "#LM2 trainer: trains LM2 with given data and a output path\n",
        "class trainModel_:\n",
        "\n",
        "    def __init__(self, dataPath, savePath, n_hidden =514, nlayers =2,\n",
        "            lr =0.001, clip =5, epochs =1, n_seqs =128, n_steps =100, dropout =0.5,\n",
        "                cuda =False):\n",
        "    \n",
        "        with open(dataPath, 'r', encoding='utf8') as f:\n",
        "            text = f.read()\n",
        "\n",
        "        chars = tuple(set(text))\n",
        "        int2char = dict(enumerate(chars))\n",
        "        char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "        encoded = np.array([char2int[ch] for ch in text])\n",
        "        batches = self.get_batches(encoded, n_seqs, n_steps)\n",
        "        x, y = next(batches)\n",
        "        net = CharRNN(chars, n_steps, n_hidden, nlayers, dropout, lr)\n",
        "        print(net)\n",
        "        \n",
        "        self.train(net, encoded, epochs, n_seqs, n_steps, lr, clip, cuda=False, print_every=1)\n",
        "        \n",
        "        checkpoint = {'n_hidden': net.n_hidden,\n",
        "                    'n_layers': net.n_layers,\n",
        "                    'state_dict': net.state_dict(),\n",
        "                    'tokens': net.chars}\n",
        "        with open(savePath, 'wb') as f:\n",
        "            torch.save(checkpoint, f)\n",
        " \n",
        " \n",
        " \n",
        "    def train(self, net, data, epochs, n_seqs, n_steps, lr, clip, cuda, print_every, val_frac=0.2):\n",
        "        ''' Training a network \n",
        "        \n",
        "            Arguments\n",
        "            ---------\n",
        "            \n",
        "            net: CharRNN network\n",
        "            data: text data to train the network\n",
        "            epochs: Number of epochs to train\n",
        "            n_seqs: Number of mini-sequences per mini-batch, aka batch size\n",
        "            n_steps: Number of character steps per mini-batch\n",
        "            lr: learning rate\n",
        "            clip: gradient clipping\n",
        "            val_frac: Fraction of data to hold out for validation\n",
        "            cuda: Train with CUDA on a GPU\n",
        "            print_every: Number of steps for printing training and validation loss\n",
        "        \n",
        "        '''\n",
        "        \n",
        "        net.train()\n",
        "        \n",
        "        opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "        \n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        \n",
        "        # create training and validation data\n",
        "        val_idx = int(len(data)*(1-val_frac))\n",
        "        data, val_data = data[:val_idx], data[val_idx:]\n",
        "        \n",
        "        if cuda:\n",
        "            net.cuda()\n",
        "        \n",
        "        counter = 0\n",
        "        n_chars = len(net.chars)\n",
        "        \n",
        "        for e in range(epochs):\n",
        "            \n",
        "            h = net.init_hidden(n_seqs)\n",
        "            \n",
        "            for x, y in self.get_batches(data, n_seqs, n_steps):\n",
        "                \n",
        "                counter += 1\n",
        "                \n",
        "                # One-hot encode our data and make them Torch tensors\n",
        "                x = one_hot_encode(x, n_chars)\n",
        "                inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                \n",
        "                if cuda:\n",
        "                    inputs, targets = inputs.cuda(), targets.cuda()\n",
        "    \n",
        "                # Creating new variables for the hidden state, otherwise\n",
        "                # we'd backprop through the entire training history\n",
        "                h = tuple([each.data for each in h])\n",
        "    \n",
        "                net.zero_grad()\n",
        "                \n",
        "                output, h = net.forward(inputs, h)\n",
        "                \n",
        "                loss = criterion(output, targets.view(n_seqs*n_steps).type(torch.LongTensor))\n",
        "    \n",
        "                loss.backward()\n",
        "                \n",
        "                # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "                nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "    \n",
        "                opt.step()\n",
        "                \n",
        "                if counter % print_every == 0:\n",
        "                    \n",
        "                    # Get validation loss\n",
        "                    val_h = net.init_hidden(n_seqs)\n",
        "                    val_losses = []\n",
        "                    \n",
        "                    for x, y in self.get_batches(val_data, n_seqs, n_steps):\n",
        "                        \n",
        "                        # One-hot encode our data and make them Torch tensors\n",
        "                        x = one_hot_encode(x, n_chars)\n",
        "                        x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                        \n",
        "                        # Creating new variables for the hidden state, otherwise\n",
        "                        # we'd backprop through the entire training history\n",
        "                        val_h = tuple([each.data for each in val_h])\n",
        "                        \n",
        "                        inputs, targets = x, y\n",
        "                        if cuda:\n",
        "                            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "    \n",
        "                        output, val_h = net.forward(inputs, val_h)\n",
        "                        val_loss = criterion(output, targets.view(n_seqs*n_steps).type(torch.LongTensor))\n",
        "                    \n",
        "                        val_losses.append(val_loss.item())\n",
        "                    \n",
        "                    print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                        \"Step: {}...\".format(counter),\n",
        "                        \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                        \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
        "                    \n",
        "\n",
        "    def get_batches(self, arr, n_seqs, n_steps):\n",
        "        '''Create a generator that returns batches of size\n",
        "        n_seqs x n_steps from arr.\n",
        "        \n",
        "        Arguments\n",
        "        ---------\n",
        "        arr: Array you want to make batches from\n",
        "        n_seqs: Batch size, the number of sequences per batch\n",
        "        n_steps: Number of sequence steps per batch\n",
        "        '''\n",
        "        \n",
        "        batch_size = n_seqs * n_steps\n",
        "        n_batches = len(arr)//batch_size\n",
        "        \n",
        "        # Keep only enough characters to make full batches\n",
        "        arr = arr[:n_batches * batch_size]\n",
        "        \n",
        "        # Reshape into n_seqs rows\n",
        "        arr = arr.reshape((n_seqs, -1))\n",
        "        \n",
        "        for n in range(0, arr.shape[1], n_steps):\n",
        "            \n",
        "            # The features\n",
        "            x = arr[:, n:n+n_steps]\n",
        "            \n",
        "            # The targets, shifted by one\n",
        "            y = np.zeros_like(x)\n",
        "            \n",
        "            try:\n",
        "                y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_steps]\n",
        "            except IndexError:\n",
        "                y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "            yield x, y\n",
        "\n",
        "\n",
        "\n",
        "#LM2 evaluater: generates and evaluates segmentations given a trained model and test data\n",
        "class evaluateModel_:\n",
        "\n",
        "    def __init__(self, testData, output, lmodel, rmodel, language, objectiveFunction, evaluate = True):\n",
        "        \n",
        "        with open(lmodel, 'rb') as f:\n",
        "            checkpoint = torch.load(f)\n",
        "        self.lloaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
        "        self.lloaded.load_state_dict(checkpoint['state_dict'])\n",
        "        \n",
        "        with open(rmodel, 'rb') as g:\n",
        "            checkpoint = torch.load(g)\n",
        "        self.rloaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
        "        self.rloaded.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "        lentropies = []\n",
        "        rentropies = []\n",
        "        util = utilities()\n",
        "        evalData = util.loadEvaluationData(testData)\n",
        "\n",
        "        for word in evalData[0]:\n",
        "            left = []\n",
        "            right = []\n",
        "            for i in range(len(word)-1):\n",
        "\n",
        "                lprobabilityDist = self.lloaded.predict(word[i])[0]\n",
        "                lcharEntropy = util.entropy(lprobabilityDist)\n",
        "                left.append(lcharEntropy)\n",
        "                rprobabilityDist = self.rloaded.predict(word[len(word)-1-i])[0]\n",
        "                rcharEntropy = util.entropy(rprobabilityDist)\n",
        "                right.append(rcharEntropy)\n",
        "\n",
        "            left.append('$')\n",
        "            right.append('$')\n",
        "            lentropies.append(left)\n",
        "            rentropies.append(right)\n",
        "\n",
        "        rentropies = util.reverseListElem(rentropies, True)\n",
        "        \n",
        "        self.prediction = util.segmenter(evalData[0], lentropies, rentropies, language, objectiveFunction)\n",
        "        if evaluate == True:\n",
        "            self.evaluation = util.evalMorphSegments(self.prediction, evalData[1])\n",
        "            self.info = (language + ':\\nTest data: ' + str(testData) + '\\nTokens: ' + str(evalData[0]) + \n",
        "                '\\nReversed tokens: ' + str(util.reverseListElem(evalData[0])) + '\\nActual segmentation: ' + \n",
        "                str(evalData[1]) + '\\nPrediction: ' + str(self.prediction) + '\\nLeft entropy: ' + \n",
        "                str(lentropies) + '\\nRight entropy: ' + str(rentropies) + '\\nScores: '+ \n",
        "                'Precision: ' + str(self.evaluation[0]) + ' Recall: ' + str(self.evaluation[1]) + \n",
        "                ' F-Score: ' + str(self.evaluation[2]) + '\\n' + '-'*10000 + '\\n')\n",
        "            with open(output, 'a') as g:\n",
        "                g.write(self.info)\n",
        "                    \n",
        "                   \n",
        "    def getScores(self):\n",
        "        return self.evaluation\n",
        "\n",
        "\n",
        "    def getInfo(self):\n",
        "        return self.info\n",
        "\n",
        "\n",
        "    def getPredictedSegmentations(self):\n",
        "        return self.prediction\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3VM1phCMROQ"
      },
      "source": [
        "## Driver functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkTIP5GIv8_3"
      },
      "source": [
        "def trainMorfessor():  \n",
        "    languages = ['ndebele', 'swati', 'xhosa', 'zulu']\n",
        "    for language in languages:\n",
        "        path = '/content/gdrive/My Drive/Colab Notebooks/colab_data/morfessor_data/'\n",
        "        m = baseline()\n",
        "        m.trainModel(path + language + '.train.txt', path + 'saved_models/' + language)\n",
        "\n",
        "\n",
        "def evalMorfessor():  \n",
        "    languages = ['ndebele', 'swati', 'xhosa', 'zulu']\n",
        "    precision, recall, f_score = 0, 0, 0\n",
        "    for language in languages:\n",
        "        path = '/content/gdrive/My Drive/Colab Notebooks/colab_data/morfessor_data/'\n",
        "        m = baseline()\n",
        "        s = m.evaluateModel(path + language + '.test.txt', path + 'results.txt', path + 'saved_models/' + language, language)\n",
        "        precision+=m.getScores()[0]\n",
        "        recall+=m.getScores()[1]\n",
        "        f_score+=m.getScores()[2]\n",
        "        print('\\nScores for ', language, ': Precsion: ', m.getScores()[0], ' Recall: ', m.getScores()[1], ' F-score: ', m.getScores()[2])\n",
        "    av = '\\nAverages: Precision: ' + str(precision/4) + ' Recall: ' + str(recall/4) + ' F-Score: ' + str(f_score/4)\n",
        "    f = open(path + 'results.txt','a')\n",
        "    f.write(av)\n",
        "    print(av)\n",
        "    \n",
        "\n",
        "def trainLM1():\n",
        "    path = '/content/gdrive/My Drive/Colab Notebooks/colab_data/entropy_data/'\n",
        "    languages = ['ndebele', 'swati', 'xhosa', 'zulu']\n",
        "    for language in languages:\n",
        "        trainModel(path + language + '_data/left/' + language + '.', path + 'LM1_models/' + language + '_left.pt')\n",
        "        trainModel(path + language + '_data/right/' + language + '.', path + 'LM1_models/' + language + '_right.pt')\n",
        "\n",
        "\n",
        "def evalLM1(objectiveFunction):\n",
        "    path = '/content/gdrive/My Drive/Colab Notebooks/colab_data/entropy_data/'\n",
        "    languages = ['ndebele', 'swati', 'xhosa', 'zulu']\n",
        "    precision, recall, f_score = 0, 0, 0\n",
        "    open(path + 'LM1_models/results.txt', 'w')\n",
        "    for language in languages:\n",
        "        s = evaluateModel(path + language + '_data/left/' + language + '.', path + language + '_data/evaluate.txt', path + 'LM1_models/results.txt', path + 'LM1_models/' + language + '_left.pt', path + 'LM1_models/' + language + '_right.pt', language, objectiveFunction)\n",
        "        print(s.getInfo())\n",
        "        precision+=s.getScores()[0]\n",
        "        recall+=s.getScores()[1]\n",
        "        f_score+=s.getScores()[2]\n",
        "    av = '\\nAverages: Precision: ' + str(precision/4) + ' Recall: ' + str(recall/4) + ' F-Score: ' + str(f_score/4)\n",
        "    f = open(path + 'LM1_models/results.txt','a')\n",
        "    f.write(av)\n",
        "    print(av)\n",
        "\n",
        "\n",
        "def trainLM2():\n",
        "    path = '/content/gdrive/My Drive/Colab Notebooks/colab_data/entropy_data/'\n",
        "    languages = ['ndebele', 'swati', 'xhosa', 'zulu']\n",
        "    for language in languages:\n",
        "        spath = path + 'LM2_models/'\n",
        "        lname = language + '_left.net'\n",
        "        lpath = path + language + '_data/left/train.txt'\n",
        "        rname = language + '_right.net'\n",
        "        rpath = path + language + '_data/right/train.txt'\n",
        "        trainModel_(lpath, spath + lname)\n",
        "        trainModel_(rpath, spath + rname)\n",
        "\n",
        "\n",
        "def evalLM2(objectiveFunction): \n",
        "    path = '/content/gdrive/My Drive/Colab Notebooks/colab_data/entropy_data/'\n",
        "    languages = ['ndebele', 'swati', 'xhosa', 'zulu']\n",
        "    precision, recall, f_score = 0, 0, 0\n",
        "    open(path + 'LM2_models/results.txt','w')\n",
        "    for language in languages:\n",
        "        s = evaluateModel_(path + language + '_data/evaluate.txt', path + 'LM2_models/results.txt', path + 'LM2_models/' + language + '_left.net', path + 'LM2_models/' + language + '_right.net', language, objectiveFunction)\n",
        "        print(s.getInfo())\n",
        "        precision+=s.getScores()[0]\n",
        "        recall+=s.getScores()[1]\n",
        "        f_score+=s.getScores()[2]\n",
        "    av = '\\nAverages: Precision: ' + str(precision/4) + ' Recall: ' + str(recall/4) + ' F-Score: ' + str(f_score/4)\n",
        "    f = open(path + 'LM2_models/results.txt','a')\n",
        "    f.write(av)\n",
        "    print(av)\n",
        "\n",
        "\n",
        "def evalInteractive(model):\n",
        "    print('Enter a list of words belonging to a single language that you would like to segment:')\n",
        "    words = input().split()\n",
        "    print('Enter the language of the words given: (zulu, xhosa, ndebele, swati) ')\n",
        "    language = input()\n",
        "    print('Enter objective function would you like to use to segment: [1-6]')\n",
        "    objectiveFunction = input()\n",
        "    print('Segmenting', words, 'belonging to', language, 'using', model,'with objective function', objectiveFunction)\n",
        "    path = '/content/gdrive/My Drive/Colab Notebooks/colab_data/entropy_data/'\n",
        "    evaluationPath = path + 'interactive.txt'\n",
        "    f = open(evaluationPath, 'w')\n",
        "    for w in words:\n",
        "        f.write(w)\n",
        "        f.write(' '+ w + '\\n')\n",
        "    f.close()\n",
        "    s = None\n",
        "    if model == 'LM1':\n",
        "        s = evaluateModel(path + language + '_data/left/' + language + '.', evaluationPath, path + 'LM1_models/results.txt', path + 'LM1_models/' + language + '_left.pt', path + 'LM1_models/' + language + '_right.pt', language, objectiveFunction, False)\n",
        "    else:\n",
        "        s = evaluateModel_(evaluationPath, path + 'LM2_models/results.txt', path + 'LM2_models/' + language + '_left.net', path + 'LM2_models/' + language + '_right.net', language, objectiveFunction, False)\n",
        "    print(s.getPredictedSegmentations())\n",
        "\n",
        "\n",
        "def evalMorfessorInteractive():\n",
        "    print('Enter a list of words belonging to a single language that you would like to segment:')\n",
        "    words = input().split()\n",
        "    print('Enter the language of the words given: (zulu, xhosa, ndebele, swati) ')\n",
        "    language = input()\n",
        "    path = '/content/gdrive/My Drive/Colab Notebooks/colab_data/morfessor_data/'\n",
        "    evaluationPath = path + 'interactive.txt'\n",
        "    f = open(evaluationPath, 'w')\n",
        "    for w in words:\n",
        "        f.write(w)\n",
        "        f.write(' '+ w + '\\n')\n",
        "    f.close()\n",
        "    b = baseline()\n",
        "    s = b.evaluateModel(evaluationPath, None, path + 'saved_models/' + language, language, False)\n",
        "    print('Segmenting', words, 'belonging to', language, 'using Morfessor\\n', s)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKsfDliy96zG"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34vc1e-X7LT2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2bd0341-cae7-4fb0-df0a-83bb29caa1f6"
      },
      "source": [
        "#print('-'*1000, '\\nMorfessor: ')\n",
        "#trainMorfessor()\n",
        "#evalMorfessor()\n",
        "#print('-'*1000, '\\nLM1: ')\n",
        "#trainLM1()\n",
        "#evalLM1(2)\n",
        "#print('-'*1000, '\\nLM2: ')\n",
        "#trainLM2()\n",
        "#evalLM2(2)\n",
        "#evalMorfessorInteractive()\n",
        "#evalInteractive('LM1')\n",
        "evalInteractive('LM2')\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a list of words belonging to a single language that you would like to segment:\n",
            "ifom yesicelo okanye\n",
            "Enter the language of the words given: (zulu, xhosa, ndebele, swati) \n",
            "xhosa\n",
            "Enter objective function would you like to use to segment: [1-6]\n",
            "2\n",
            "Segmenting ['ifom', 'yesicelo', 'okanye'] belonging to xhosa using LM2 with objective function 2\n",
            "[['i', 'fom'], ['yes', 'ic', 'elo'], ['o', 'kanye']]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}